{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\manas\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "import cohere\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pdfplumber\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone and Cohere clients\n",
    "pc = Pinecone(api_key=\"7503c7e4-ecbb-43b8-8a06-920f4281ff21\", environment=\"us-east-1\")\n",
    "co = cohere.Client(\"RcO7I05QJgF7t44biTgY85UPxFfUPISCbXPFrRvN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index name\n",
    "index_name = \"sample-article\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    pc.create_index(name=index_name, dimension=384, metric=\"cosine\", spec=spec)\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained SentenceTransformer for embedding generation\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_file) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + \" \"\n",
    "        print(f\"Successfully extracted {len(text)} characters from PDF\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {str(e)}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_file):\n",
    "    text = extract_text_from_pdf(pdf_file)\n",
    "    chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "    print(f\"Created {len(chunks)} chunks from the document\")\n",
    "    \n",
    "    embeddings = embedder.encode(chunks)\n",
    "    print(f\"Generated embeddings for {len(embeddings)} chunks\")\n",
    "    \n",
    "    try:\n",
    "        vectors_to_upsert = [(f\"chunk_{i}\", emb.tolist(), {\"text\": chunk}) for i, (emb, chunk) in enumerate(zip(embeddings, chunks))]\n",
    "        index.upsert(vectors=vectors_to_upsert)\n",
    "        print(f\"Successfully uploaded {len(embeddings)} embeddings to Pinecone\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading embeddings to Pinecone: {str(e)}\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=3):\n",
    "    query_embedding = embedder.encode([query])[0]\n",
    "    try:\n",
    "        query_results = index.query(vector=query_embedding.tolist(), top_k=top_k, include_metadata=True)\n",
    "        print(f\"Retrieved {len(query_results['matches'])} relevant chunks\")\n",
    "        return [match['metadata']['text'] for match in query_results['matches'] if 'metadata' in match and 'text' in match['metadata']]\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Pinecone: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, context):\n",
    "    try:\n",
    "        response = co.generate(\n",
    "            model='command',\n",
    "            prompt=f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "            max_tokens=100\n",
    "        )\n",
    "        print(\"Successfully generated answer using Cohere\")\n",
    "        return response.generations[0].text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer with Cohere: {str(e)}\")\n",
    "        return \"Sorry, I couldn't generate an answer at this time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_bot(pdf_file, question):\n",
    "    if pdf_file is None:\n",
    "        return \"Please upload a PDF file.\"\n",
    "    \n",
    "    document_chunks = process_document(pdf_file.name)  # Use pdf_file.name to get the file path\n",
    "    relevant_chunks = retrieve_relevant_chunks(question)\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 2341 characters from PDF\n",
      "Created 5 chunks from the document\n",
      "Generated embeddings for 5 chunks\n",
      "Successfully uploaded 5 embeddings to Pinecone\n",
      "Retrieved 3 relevant chunks\n",
      "Successfully generated answer using Cohere\n",
      "Successfully extracted 2341 characters from PDF\n",
      "Created 5 chunks from the document\n",
      "Generated embeddings for 5 chunks\n",
      "Successfully uploaded 5 embeddings to Pinecone\n",
      "Retrieved 3 relevant chunks\n",
      "Successfully generated answer using Cohere\n",
      "Successfully extracted 10733 characters from PDF\n",
      "Created 22 chunks from the document\n",
      "Generated embeddings for 22 chunks\n",
      "Successfully uploaded 22 embeddings to Pinecone\n",
      "Retrieved 3 relevant chunks\n",
      "Successfully generated answer using Cohere\n",
      "Created dataset file at: .gradio\\flagged\\dataset2.csv\n",
      "Successfully extracted 10733 characters from PDF\n",
      "Created 22 chunks from the document\n",
      "Generated embeddings for 22 chunks\n",
      "Successfully uploaded 22 embeddings to Pinecone\n",
      "Retrieved 3 relevant chunks\n",
      "Successfully generated answer using Cohere\n",
      "Successfully extracted 10733 characters from PDF\n",
      "Created 22 chunks from the document\n",
      "Generated embeddings for 22 chunks\n",
      "Successfully uploaded 22 embeddings to Pinecone\n",
      "Retrieved 3 relevant chunks\n",
      "Successfully generated answer using Cohere\n"
     ]
    }
   ],
   "source": [
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=qa_bot,\n",
    "    inputs=[\n",
    "        gr.File(label=\"Upload PDF Document\", file_types=[\".pdf\"]),\n",
    "        gr.Textbox(label=\"Ask a Question\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Interactive QA Bot\",\n",
    "    description=\"Upload a PDF document and ask a question based on its contents.\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
