{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\manas\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone and Cohere clients\n",
    "pc = Pinecone(\n",
    "    api_key=\"7503c7e4-ecbb-43b8-8a06-920f4281ff21\",\n",
    "    environment=\"us-east-1\"  # Change this to your Pinecone environment\n",
    ")\n",
    "co = cohere.Client(cohere_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the index name\n",
    "index_name = \"sample-article\"\n",
    "\n",
    "# Create Pinecone index if it doesn't exist\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    spec = ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\manas\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained SentenceTransformer for embedding generation\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(source_type=\"text\", source=None, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Load a document from a local file, URL, or raw text, and split it into chunks.\n",
    "\n",
    "    Args:\n",
    "        source_type (str): The type of source ('file', 'url', 'text').\n",
    "        source (str): The path to the file, URL, or raw text.\n",
    "        chunk_size (int): The size of each document chunk (in characters).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of document chunks.\n",
    "    \"\"\"\n",
    "    if source_type == \"file\":\n",
    "        with open(source, 'r', encoding='utf-8') as f:\n",
    "            document = f.read()\n",
    "    elif source_type == \"url\":\n",
    "        import requests\n",
    "        response = requests.get(source)\n",
    "        document = response.text\n",
    "    elif source_type == \"text\":\n",
    "        document = source\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported source_type. Choose from 'file', 'url', or 'text'.\")\n",
    "    \n",
    "    # Split document into chunks\n",
    "    chunks = [document[i:i+chunk_size] for i in range(0, len(document), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Load a document from raw text\n",
    "document_chunks = load_document(source_type=\"text\", source=\"chunking.txt\", chunk_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate embeddings for each chunk\n",
    "embeddings = embedder.encode(document_chunks, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Upload embeddings to Pinecone\n",
    "for i, chunk in enumerate(document_chunks):\n",
    "    index.upsert([(str(i), embeddings[i].numpy())])  # Upsert embeddings with ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant documents\n",
    "def retrieve_relevant_chunks(query, top_k=3):\n",
    "    query_embedding = embedder.encode([query])[0]  # Embed the query\n",
    "    query_results = index.query(vector=[0.1, 0.2, 0.3], top_k=10, namespace='my_namespace') # Query Pinecone\n",
    "    relevant_chunks = [document_chunks[int(match['id'])] for match in query_results['matches']]\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cohere Generation Function\n",
    "def generate_answer(question, context):\n",
    "    response = co.generate(\n",
    "        model='command',  # Choose your Cohere model size\n",
    "        prompt=f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\",\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.generations[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full QA function (Retrieve + Generate)\n",
    "def answer_question(question):\n",
    "    # 1. Retrieve relevant chunks\n",
    "    relevant_chunks = retrieve_relevant_chunks(question)\n",
    "    # 2. Combine relevant chunks\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    # 3. Generate answer\n",
    "    answer = generate_answer(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the chunking?\n",
      "Answer:  The process of grouping individual pieces of information together in order to make them easier to memorize or retain. Chunking is a technique often used in cognitive psychology and language acquisition to improve memory and increase the amount one can remember. It works by organizing information into more easily learned groups, phrases, words, or numbers.\n",
      "\n",
      "For example, it is easier to memorize this sequence of numbers: 1492-2006-93-15 than it is to remember these separate dates: 1492, 2006,\n",
      "------------------------------------------------\n",
      "Question: What are all the types of chunking can you give them and compare each of them in a tabular manner?\n",
      "Answer:  Below is a comparison of the three types of chunking, outlining their defining characteristics, application in natural language processing(NLP) and language modeling (LM), along with their corresponding advantages and disadvantages. \n",
      "1. **Phonetic Chunking**:\n",
      "- Definition: Phonetic chunking involves grouping words based on their phonetic similarity, emphasizing how they sound, rather than their semantic meaning or spelling. \n",
      "- NLP Application: Phonetic chunking aids in tackling tasks like automatic speech\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample query\n",
    "sample_question = \"What is the chunking?\"\n",
    "answer = answer_question(sample_question)\n",
    "print(f\"Question: {sample_question}\\nAnswer: {answer}\")\n",
    "print(\"------------------------------------------------\")\n",
    "sample_question = \"What are all the types of chunking can you give them and compare each of them in a tabular manner?\"\n",
    "answer = answer_question(sample_question)\n",
    "print(f\"Question: {sample_question}\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
